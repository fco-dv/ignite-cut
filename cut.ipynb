{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLhpLFViHhyjVjw6IPLin6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fco-dv/ignite-cut/blob/main/cut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLZEOH4uaplk"
      },
      "source": [
        "# Contrastive Learning for UnpairedImage-to-Image Translation (CUT) with Ignite and `torch.cuda.amp`\n",
        "\n",
        "In this notebook we provide an implementation of [CUT](https://arxiv.org/pdf/2007.15651) and its training on \"Horse 2 Zebra\" dataset using Ignite.\n",
        "\n",
        "In contrast, we will use recently added [`torch.cuda.amp`](https://pytorch.org/docs/master/notes/amp_examples.html#working-with-multiple-models-losses-and-optimizers) module to perform automatic mixed precision training instead of using Nvidia/Apex package. This module is available in pytorch (>=1.6.0) release.\n",
        "\n",
        "\n",
        "### CUT in a Nutshell\n",
        "\n",
        "CUT only requires learning the mapping in one direction and avoids using inverse auxiliary generators and discriminators. This can largely simplify the training procedure and reduce training time. The generator function $G$ is broken up into two components, an encoder $Genc$ followed by a decoder $Gdec$, which are applied sequentially to produce output image.\n",
        "\n",
        "$$\n",
        "\\hat{y} = G(x) = Gdec(Genc(x))\n",
        "$$\n",
        "\n",
        "\n",
        "and two discriminators $D_A$ and $D_B$. Training of the networks is done by minimizing the loss is a sum of 3 components:\n",
        "$$\n",
        "\\mathcal{L}(G, F, D_A, D_B) = \\mathcal{L}_{GAN}(G, D_B) + \\mathcal{L}_{GAN}(F, D_A) + \\lambda \\mathcal{L}_{cyc}(G, F)\n",
        "$$\n",
        "with GAN loss:\n",
        "$$\n",
        "\\mathcal{L}_{GAN}(G, D_B) = \\text{mean}_{x \\in A}\\left[ (D_B(G(x)) - 1)^2 \\right]+ \\text{mean}_{y \\in B}\\left[ (D_B(y) - 1)^2 \\right] \\\\\n",
        "\\mathcal{L}_{GAN}(F, D_A) = \\text{mean}_{y \\in B}\\left[ (D_A(F(y)) - 1)^2 \\right]+ \\text{mean}_{x \\in A}\\left[ (D_A(x) - 1)^2 \\right]\n",
        "$$\n",
        "and forward and backward cycle consistency term:\n",
        "$$\n",
        "\\mathcal{L}_{cyc}(G, F) = \\text{mean}_{x \\in A}\\left[ |F(G(x)) - x|_1 \\right] + \\text{mean}_{y \\in B}\\left[ |G(F(y)) - y|_1 \\right]\n",
        "$$\n",
        "\n",
        "Optionally, one can add identity loss terms. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#what-is-the-identity-loss-322-373-362)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfMtNKDbYPyS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "f59a15a0-a7b2-4396-ba44-784f5b2a2ed1"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-4018dd74e384>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    In this notebook we provide an implementation of [CUT](https://arxiv.org/pdf/2007.15651) and its training on \"Horse 2 Zebra\" dataset using Ignite.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB8A7Q--ak1w"
      },
      "source": [
        ""
      ]
    }
  ]
}