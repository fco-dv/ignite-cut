{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9FwIlHBAFgLSXP5Z1Mf4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fco-dv/ignite-cut/blob/main/cut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfMtNKDbYPyS"
      },
      "source": [
        "# Contrastive Learning for UnpairedImage-to-Image Translation (CUT) with Ignite and `torch.cuda.amp`\n",
        "\n",
        "In this notebook we provide an implementation of [CUT](https://arxiv.org/pdf/2007.15651) and its training on \"Horse 2 Zebra\" dataset using Ignite. This notebook is almost similar to another our [notebook on CycleGAN with Nvidia/Apex](https://github.com/pytorch/ignite/blob/master/examples/notebooks/CycleGAN_with_ignite_and_nvdia_apex.ipynb).\n",
        "\n",
        "In contrast, we will use recently added [`torch.cuda.amp`](https://pytorch.org/docs/master/notes/amp_examples.html#working-with-multiple-models-losses-and-optimizers) module to perform automatic mixed precision training instead of using Nvidia/Apex package. This module is available in pytorch (>=1.6.0) release.\n",
        "\n",
        "\n",
        "### CycleGAN in a Nutshell\n",
        "\n",
        "CycleGAN is unpaired image-to-image translation task from $A$ to $B$ and represented by two generative networks $G$ and $F$:\n",
        "$$\n",
        "\\hat{y} = G(x) \\in B,\\text{ for } x \\in A \\\\\n",
        "\\hat{x} = F(y) \\in A,\\text{ for } y \\in B\n",
        "$$\n",
        "\n",
        "and two discriminators $D_A$ and $D_B$. Training of the networks is done by minimizing the loss is a sum of 3 components:\n",
        "$$\n",
        "\\mathcal{L}(G, F, D_A, D_B) = \\mathcal{L}_{GAN}(G, D_B) + \\mathcal{L}_{GAN}(F, D_A) + \\lambda \\mathcal{L}_{cyc}(G, F)\n",
        "$$\n",
        "with GAN loss:\n",
        "$$\n",
        "\\mathcal{L}_{GAN}(G, D_B) = \\text{mean}_{x \\in A}\\left[ (D_B(G(x)) - 1)^2 \\right]+ \\text{mean}_{y \\in B}\\left[ (D_B(y) - 1)^2 \\right] \\\\\n",
        "\\mathcal{L}_{GAN}(F, D_A) = \\text{mean}_{y \\in B}\\left[ (D_A(F(y)) - 1)^2 \\right]+ \\text{mean}_{x \\in A}\\left[ (D_A(x) - 1)^2 \\right]\n",
        "$$\n",
        "and forward and backward cycle consistency term:\n",
        "$$\n",
        "\\mathcal{L}_{cyc}(G, F) = \\text{mean}_{x \\in A}\\left[ |F(G(x)) - x|_1 \\right] + \\text{mean}_{y \\in B}\\left[ |G(F(y)) - y|_1 \\right]\n",
        "$$\n",
        "\n",
        "Optionally, one can add identity loss terms. See [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#what-is-the-identity-loss-322-373-362)."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}